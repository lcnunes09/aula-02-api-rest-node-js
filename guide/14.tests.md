# Type of Tests

**Unit Tests:**
*Definition*: Unit tests are focused on testing individual components or functions in isolation. A unit test typically targets a small unit of code, like a function, method, or class.
*Purpose*: They help ensure that each part of your code works as expected and meets its specifications.
*Characteristics*: Unit tests are fast, isolated, and should not have external dependencies or interactions with databases, networks, or other services.

**Integration Tests:**
*Definition*: Integration tests focus on testing interactions between different components or modules of a system. They verify that different parts of your application work together as expected.
*Purpose*: They catch issues that might arise when different components are integrated, such as incorrect data exchange or communication problems.
*Characteristics*: Integration tests might involve multiple units, databases, external services, and other components. They are broader in scope compared to unit tests.

**End-to-End Tests (E2E Tests):**
*Definition*: End-to-end tests simulate real user scenarios by testing the entire application from start to finish. They mimic user interactions and verify the flow and behavior of the application.
*Purpose*: E2E tests help ensure that the entire application works as expected and that various parts of the system interact correctly.
*Characteristics*: E2E tests cover the full stack, including the frontend, backend, and often external services. They tend to be slower and more complex to set up compared to unit and integration tests.

Unit tests catch issues at a granular level, integration tests ensure components work together, and end-to-end tests verify the complete application's functionality.

In practice, a comprehensive testing strategy often involves a combination of these test types to ensure that your software is robust, reliable, and meets its requirements.

For this application, we are going to use **Vitest**.

## Install Vitest

Install as a development dependency, since the tests are never going to be run in Production:

```bash
npm i vitest -D
```

Create a `test` folder in the root of your application. This is where your tests are going to live.

To facilitate to run tests, create a new script on `package.json` file:

```json
"test": "vitest"
```

To run the tests - after implementing it:

```bash
npm run test
```

OR

```bash
npm test
```

## Supertest

Using Supertest with Fastify streamlines the process of testing your Fastify application's endpoints and ensures that your API behaves as expected. It provides a structured and efficient way to perform integration testing, helping you catch bugs and issues before they reach production.

### Install Supertest

Install as a development dependency, since the tests are never going to be run in Production:

```bash
npm i supertest -D
```

By using `supertest` means that you can run the test without running the application. With that said, it will be necessary to separate the app configuration and the code to run the server.

Note: Supertest is a library created in javascript, as is possible to see [here](https://www.npmjs.com/package/supertest). Because of that, it is also necessary to install @types/supertest.

```bash
npm i -D @types/supertest
```

## Create app.ts and update server.ts

Create `app.ts` file inside `src` folder.

On the `app.ts` file, add everything that is before to the `listen` method and **export** the *app*:

```js
import fastify from 'fastify'
import { transactionsRoutes } from './routes/transactions'
import cookie from '@fastify/cookie'

export const app = fastify()

app.register(cookie)

app.register(transactionsRoutes, {
    prefix: 'transactions',
})
```

At `server.ts` file, import *app* and *env* variables and add the `listen` method:

```js
import { app } from './app'
import { env } from './env' 

app.listen({
    port: env.PORT,
}).then(() => {
    console.log('HTTP Server Running')
})
```

Now, when importing the `app.ts` in the tests files, it won't be running the application, only the tests themselves.

## Create first test file

Create a file `transactions.spec.ts` inside test folder:

```js
import { test, beforeAll, afterAll } from 'vitest'
import request from 'supertest'
import { app } from '../src/app'

beforeAll(async () => {
    await app.ready()
})

afterAll(async () => {
    await app.close()
})

test('user can create a new transaction', async () => {
    const response = await request(app.server)
        .post('/transactions')
        .send({
            title: 'New transaction',
            amount: 5000,
            type: 'credit',
        })
        .expect(201)
})
```

## Categorizing tests using Vitest

To better categorize and group your tests, use the `describe` functionality in Vistest. This will help to know which file has failing test and the category.

Also, to get a better semantic with the test scenarios, it's good to use `it` instead of `test` to have the sense of `it should be able to...`.

Update the file `transactions.spec.ts`:

```js
import { it, beforeAll, afterAll, describe } from 'vitest'
import request from 'supertest'
import { app } from '../src/app'


describe('Transactions routes', () => {
    beforeAll(async () => {
        await app.ready()
    })

    afterAll(async () => {
        await app.close()
    })

    it('it should be able to create a new transaction', async () => {
        const response = await request(app.server)
            .post('/transactions')
            .send({
                title: 'New transaction',
                amount: 5000,
                type: 'deposit',
            })
            .expect(201)
    })
})
```

## Tips and Tricks when writing tests

```js
it.todo('should be able to list all transactions', async () => {})
```

`it.todo`: to remember to define a test later. It will show that there is a `todo` test when you run the tests.

`it.skip`: to skip a test.

`it.only`: to run only one test.

## Test concepts

Principle of test isolation and independence: all tests should be agnostic/excluded from any context. It's a fundamental concept in testing, and it means that each test case should be self-contained and not rely on the state or outcomes of other tests.

**Test Isolation**: Each test should operate in isolation, meaning it should not depend on the execution or success of other tests. This ensures that a failure in one test does not cascade and affect the results of subsequent tests. Isolation helps pinpoint the exact cause of a test failure.

**No Hidden Dependencies**: Tests should not rely on hidden or external dependencies that are not explicitly included in the test setup. Hidden dependencies can make tests brittle and prone to breaking when external factors change.

**Clear Setup and Teardown**: Tests should set up their required context or state before execution and clean up after themselves when done. This ensures that the test environment is in a known state before the test starts and is restored to a clean state afterward.

**Consistency**: Tests should produce consistent results regardless of when or how often they are run. They should not produce different outcomes due to the order of execution or external factors.

**Parallel Execution**: Independent tests can be run in parallel without interfering with each other. This can significantly speed up the test suite's execution.

**Reproducibility**: Tests should be reproducible, meaning they should produce the same results each time they are run. This is essential for debugging and identifying issues.

**Maintenance**: Isolated tests are easier to maintain because changes in one test are less likely to impact others. Developers can confidently modify and refactor code without worrying about unintended consequences on the test suite.

**Clear Failure Diagnosis**: When a test fails, it should be easy to identify the cause. Isolated tests make it clear whether the issue is within the specific test case or in the code being tested.

## Create list transactions test

In order to create the test to list transactions and based on the concept above, it will be necessary to execute the steps since the begining (aka create a transaction) before listing the transactions.

```js
it('should be able to list all transactions', async () => {
    const createTransactionResponse = await request(app.server)
        .post('/transactions')
        .send({
            title: 'New transaction',
            amount: 500,
            type: 'deposit'
        })

    const cookies = createTransactionResponse.get('Set-Cookie')

    const listTransactionsResponse = await request(app.server)
        .get('/transactions')
        .set('Cookie', cookies)
        .expect(200)

    expect(listTransactionsResponse.body.transactions).toEqual([
        expect.objectContaining({
            title: 'New transaction',
            amount: 500,
        }),
    ])
})
```

Note: Remember to add the `expect` into the import!

There are different ways to set the content to expect.

- Important to know that in this case we expect to have an object, inside the object there are the transactions, and then an array. That's why to expect `listTransactionsResponse` needs to have body AND transactions.
- We are only adding title and amount because they are saved into the database. The type is not saved so is not returned into the list.
- Another way of doing is to check if there is any String into the id, which would mean that a transaction was created. It's good when you don't know the data, for example, created_at that is auto generated.

```js
expect(listTransactionsResponse.body).toEqual([
    id: expect.any(String)
])
```

## Testing Database

Creating a completely new database for tests is a common practice in software testing for several important reasons:

**Isolation**: Tests should be isolated from each other and from the production environment. Using a separate test database ensures that changes made during testing do not affect the production data. It prevents accidental modifications or deletions of real data, which could be costly or disastrous.

**Predictable State**: Tests often rely on a predictable initial state. When you create a new database for tests, you start with a clean slate, and you can set up the initial state precisely as needed for each test case. This ensures that tests are consistent and that you know the exact state of the data at the beginning of each test.

**Repeatability**: Tests should be repeatable, meaning they should produce the same results each time they are run. A separate test database allows you to reset the data to a known state before each test, ensuring that tests are reproducible.

**Parallel Testing**: Running tests in parallel can significantly speed up the test suite's execution. With a dedicated test database for each test runner, you avoid contention for database resources, enabling parallel testing without conflicts.

**Avoiding Side Effects**: Tests should not have side effects on the production environment. If tests were allowed to modify the production database, they could inadvertently impact the behavior of the live application or disrupt its functionality.

**Data Privacy**: In many cases, production data contains sensitive information that should not be exposed to test environments. Using a separate test database helps maintain data privacy and compliance with regulations.

**Cleanup**: After running tests, it's essential to clean up any data created or modified during testing. With a separate test database, you can easily wipe out the test data without affecting the production data.

**Confidence**: Knowing that your tests are working with controlled, isolated data gives you confidence in the test results. You can be sure that test failures are not due to interactions with external data or factors.

To implement a separate test database effectively, you'll often see practices like:

**Database Migrations**: Automatically apply database schema changes for the test database to mirror the production database's structure.

**Test Data Setup**: Use scripts, fixtures, or factories to populate the test database with the necessary data for each test case.

**Teardown**: Clean up the test database after each test run to remove any changes made during testing.

**CI/CD Integration**: Incorporate the creation and management of test databases into your continuous integration and continuous deployment (CI/CD) pipelines to ensure consistency across environments.

## Create a separate environment configuration to run your test

Create a `.env.test` file and add it to the `.gitignore`.

On the `.env.test` file you will add the variables only for the test environment.

```env
DATABASE_URL="./db/test.db"
```

Note: make sure to create the file `.env.test.example`.

Important: When using a tool like Vitest, it's **not** necessary to declare NODE_ENV (`NODE_ENV=test`) because it is already setup by Vistest.

Changes on the `index.ts` file:

- When importing `dotenv.config`, this means that will import exactly the `.env` file. In order to differ depending on the environment, it's necessary to change the way to import and to validate if the NODE_ENV is test or not:

```js
import { config } from 'dotenv'

if (process.env.NODE_ENV === 'test') {
    config({ path: '.env.test' })
} else {
    config()
}
```

## Database Migrations on database test

Since we are creating a new database for our tests, we need to clear the database and execute the migrations before each test scenario. For that to happen, we will use the `beforeEach` function.

```js
import { execSync } from 'node:child_process'

beforeEach(() => {
    execSync('npm run knex migrate:rollback --all')
    execSync('npm run knex migrate:latest')
})
```

Note: The execSync function from the child_process module in Node.js is a synchronous way to execute shell commands or external processes from within a Node.js script. It is part of the child process module and is used for running shell commands and obtaining the results in a synchronous manner.

## Create get a specific transaction test

Note that to get a specific transaction it's not inside an array, so we need to remove that on the `expect`, and we need to create the transaction, get the transaction id and check that existing transaction.

```js
it('should be able to get a specific transaction', async () => {
    const createTransactionResponse = await request(app.server)
        .post('/transactions')
        .send({
            title: 'New transaction',
            amount: 500,
            type: 'deposit'
        })

    const cookies = createTransactionResponse.get('Set-Cookie')

    const listTransactionsResponse = await request(app.server)
        .get('/transactions')
        .set('Cookie', cookies)
        .expect(200)

    const transactionId = listTransactionsResponse.body.transactions[0].id

    const getTransactionResponse = await request(app.server)
        .get(`/transactions/${transactionId}`)
        .set('Cookie', cookies)
        .expect(200)

    expect(getTransactionResponse.body.transaction).toEqual(
        expect.objectContaining({
            title: 'New transaction',
            amount: 500,
        }),
    )
})
```
